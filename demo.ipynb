{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this is nothing in results_folder\n",
    "\n",
    "# ! python train.py \"data/Cambridge_KingsCollege/train/rgb/*.png\" results_folder/ace_network.pt --pose_files \"data/Cambridge_KingsCollege/train/poses/*.txt\"  --pose_refinement none --use_external_focal_length 740 --refine_calibration False --use_aug False --patch_threshold 0.1\n",
    "\n",
    "# ! python export_point_cloud.py  results_folder/point_cloud_out.txt --network results_folder/ace_network.pt  --pose_file results_folder/poses_ace_network_preliminary.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Scene Coordinate Prediction and Pose Estimation Demo\n",
    "\n",
    "This demo showcases how to use a pre-trained neural network to predict scene coordinates and estimate camera pose using the DSAC\\* algorithm. We will load point cloud data, process an image, perform inference, and visualize the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "from vis_3d import init_figure, plot_points, plot_camera\n",
    "from ace_pose.network import Regressor\n",
    "from ace_pose.dataset import CamLocDataset\n",
    "import numpy as np\n",
    "import dsacstar\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Point Cloud Data\n",
    "\n",
    "First, we load the previously generated point cloud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load point cloud data\n",
    "point_cloud_3d = np.loadtxt(\"results_folder/point_cloud_out.txt\")\n",
    "# Not too mush for vis.\n",
    "random_indices = np.random.choice(point_cloud_3d.shape[0], 10000, replace=False)\n",
    "point_cloud_3d = point_cloud_3d[random_indices, :]\n",
    "pc_xyz = point_cloud_3d[..., :3]\n",
    "\n",
    "# Convert from OpenGL coordinate system to OpenCV coordinate system\n",
    "pc_xyz[:, 1] = -pc_xyz[:, 1]\n",
    "pc_xyz[:, 2] = -pc_xyz[:, 2]\n",
    "\n",
    "# Color information\n",
    "pc_color = point_cloud_3d[..., 3:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Preprocess Image\n",
    "\n",
    "We load the test image from the dataset and perform necessary preprocessing, such as resizing and setting intrinsic parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image path\n",
    "queue_image_path = \"data/Cambridge_GreatCourt/test/rgb/seq1_frame00564.png\"\n",
    "gt_pose_path = \"data/Cambridge_GreatCourt/test/poses/seq1_frame00564.txt\"\n",
    "gt_pose = np.loadtxt(gt_pose_path)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = CamLocDataset(queue_image_path, image_short_size=480)\n",
    "test_dataset.set_external_focal_length(740)\n",
    "\n",
    "# Retrieve data\n",
    "image_1HW, _, _, _, intrinsics_33, _, _, filename, indice = test_dataset[0]\n",
    "\n",
    "# Open original image\n",
    "image_rgb = Image.open(queue_image_path)\n",
    "\n",
    "# Get original size\n",
    "original_size = image_rgb.size\n",
    "width, height = original_size\n",
    "\n",
    "# Resize image\n",
    "new_size = (width // 8, height // 8)\n",
    "resized_image_rgb = np.asarray(image_rgb.resize(new_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained Network Models\n",
    "\n",
    "We load the pre-trained weights for the encoder and head networks, and set up the model for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network weight paths\n",
    "encoder_path = \"xfeat.pt\"\n",
    "head_network_path = \"results_folder/ace_network.pt\"\n",
    "\n",
    "# Load network weights\n",
    "encoder_state_dict = torch.load(encoder_path, map_location=\"cpu\")\n",
    "head_state_dict = torch.load(head_network_path, map_location=\"cpu\")\n",
    "\n",
    "# Create regressor\n",
    "network = Regressor.create_from_split_state_dict(encoder_state_dict, head_state_dict)\n",
    "\n",
    "# Move to GPU and set to evaluation mode\n",
    "network = network.to('cuda')\n",
    "network.eval()\n",
    "\n",
    "# Disable gradient computation and move image to GPU\n",
    "with torch.no_grad():\n",
    "    image_1HW = image_1HW.to('cuda', non_blocking=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Scene Coordinates\n",
    "\n",
    "Using the neural network, we perform inference on the input image to predict 3D scene coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Perform inference with automatic mixed precision\n",
    "    with autocast(enabled=True):\n",
    "        scene_coordinates_3HW = network(image_1HW.unsqueeze(0))[0]\n",
    "        _, heatmap_1HW = network.get_features(image_1HW.unsqueeze(0))\n",
    "    # Move to CPU and convert to float\n",
    "    scene_coordinates_3HW = scene_coordinates_3HW.float().cpu()\n",
    "    \n",
    "    # Extract intrinsic parameters\n",
    "    focal_length = intrinsics_33[0, 0].item()\n",
    "    ppX = intrinsics_33[0, 2].item()\n",
    "    ppY = intrinsics_33[1, 2].item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Convert scene coordinates to NumPy array\n",
    "sc_np = scene_coordinates_3HW.permute(1, 2, 0).numpy()\n",
    "heatmap_np = heatmap_1HW[0, 0].cpu().numpy()\n",
    "\n",
    "normalized_sc = np.zeros_like(sc_np)\n",
    "\n",
    "# Normalize each channel separately\n",
    "for i in range(3):\n",
    "    lower = np.percentile(sc_np[:, :, i], 25)\n",
    "    upper = np.percentile(sc_np[:, :, i], 75)\n",
    "    temp = (sc_np[:, :, i] - lower) / (upper - lower)\n",
    "    temp[heatmap_np < 0.1] = 0\n",
    "    normalized_sc[:, :, i] = temp\n",
    "\n",
    "# Clip values to keep them in the range [0, 1]\n",
    "normalized_sc = np.clip(normalized_sc, 0, 1)\n",
    "\n",
    "# Display original image\n",
    "axes[0].imshow(image_rgb)\n",
    "axes[0].set_title(\"Original Image\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Display normalized scene coordinates\n",
    "axes[1].imshow(normalized_sc)\n",
    "axes[1].set_title(\"Scene Coordinates for Useful Pixel\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Pose Using DSAC\\*\n",
    "\n",
    "We use the DSAC\\* algorithm to compute the camera pose based on the predicted scene coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize output pose matrix\n",
    "out_pose = torch.zeros((4, 4))\n",
    "\n",
    "# Estimate pose using DSAC*\n",
    "inlier_map = dsacstar.forward_rgb(\n",
    "    scene_coordinates_3HW.unsqueeze(0),\n",
    "    out_pose,\n",
    "    128,                    # Maximum iterations\n",
    "    5,                    # Inlier threshold\n",
    "    focal_length,\n",
    "    ppX,\n",
    "    ppY,\n",
    "    100,                   # RANSAC threshold\n",
    "    100,                   # RANSAC max iterations\n",
    "    network.OUTPUT_SUBSAMPLE,\n",
    "    42                   # Random seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Visualization\n",
    "\n",
    "Visualize the scene coordinates, camera pose, and point cloud data in a 3D plot.\n",
    "\n",
    "You may need to use your mouse to adjust the perspective to see everything clearly.\n",
    "\n",
    "The red dots show the queue image scene coordinates used for PnP.\n",
    "\n",
    "Red camera is the queue pose.\n",
    "\n",
    "Green camera is the ground-truth pose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize 3D figure\n",
    "fig = init_figure()\n",
    "\n",
    "# Plot original point cloud\n",
    "plot_points(fig, pc_xyz, pc_color, name='Training Set Point Cloud')\n",
    "\n",
    "inlier_sc_3I = scene_coordinates_3HW.view(3, -1)[:, inlier_map.flatten().bool()]\n",
    "resized_image_rgb.reshape((-1, 3))\n",
    "# Plot predicted scene coordinates with corresponding colors\n",
    "plot_points(fig, inlier_sc_3I.permute(1, 0).numpy(), name='Queue Image Scene Coordinates')\n",
    "\n",
    "# Plot camera pose\n",
    "plot_camera(\n",
    "    fig, \n",
    "    R=out_pose[:3, :3].numpy(), \n",
    "    t=out_pose[:3, 3].numpy(), \n",
    "    K=intrinsics_33.numpy(), \n",
    "    color='rgb(255, 0, 0)', \n",
    "    size=20, \n",
    "    text='predicted_pose'\n",
    ")\n",
    "\n",
    "plot_camera(\n",
    "    fig, \n",
    "    R=gt_pose[:3, :3], \n",
    "    t=gt_pose[:3, 3], \n",
    "    K=intrinsics_33.numpy(), \n",
    "    color='rgb(0, 255, 0)', \n",
    "    size=20, \n",
    "    text='gt_pose'\n",
    ")\n",
    "# Display the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ace0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
